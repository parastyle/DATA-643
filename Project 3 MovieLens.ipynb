{"cells":[{"cell_type":"code","source":["#Two datasets found - A big and small dataset provided by Movielens \ncomplete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\nsmall_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\ndata_path = '/dbfs/FileStore/data'\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import urllib\nsmall_f = urllib.urlretrieve (small_dataset_url, '/dbfs/FileStore/small.zip')\ncomplete_f = urllib.urlretrieve (complete_dataset_url, '/dbfs/FileStore/complete.zip')\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import zipfile\n\nwith zipfile.ZipFile('/dbfs/FileStore/small.zip', \"r\") as z:\n    z.extractall(data_path)\n\nwith zipfile.ZipFile('/dbfs/FileStore/complete.zip', \"r\") as z:\n    z.extractall(data_path)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import os\nos.listdir(data_path+'/ml-latest-small')\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["os.listdir(data_path+'/ml-latest')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import pandas as pd\npandas_df = pd.read_csv(\"/dbfs/FileStore/data/ml-latest-small/ratings.csv\", header=0)\nsmall_ratings = sqlContext.createDataFrame(pandas_df).drop('timestamp').rdd.map(tuple)\npandas_df = pd.read_csv(\"/dbfs/FileStore/data/ml-latest-small/movies.csv\",header=0)\nsmall_movies = sqlContext.createDataFrame(pandas_df).drop('genres').rdd.map(tuple)\n\n#I remove timestamp from the ratings file and genres from the movies file while converting csv to RDD.\n\n#While there was no 'real' point to first converting csv into pandas DF to then spark's RDDs; it read much easier for me.\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["small_ratings.take(3)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["small_movies.take(2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["new_user_ID = 0\nnew_user_ratings = [\n     (0,1,5), #Toy story\n     (0,2,4), # Jumanji\n     (0,3,3), # Grumpier old men\n     (0,4,2), # Waiting to exhale\n     (0,5,2), # Father of bride pt. 2\n     (0,6,1), # Heat\n     (0,7,5), # Sabrina \n     (0,8,4), #Tom and huck \n     (0,9,3) , # sudden death\n     (0,10,3) # golden eye\n    ]\nsmall_ratings = small_ratings.union(sc.parallelize(new_user_ratings))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["small_training,small_validation,small_test = small_ratings.randomSplit([6.0,2.0,2.0],seed = 26)\nsmall_validation_predict = small_validation.map(lambda x: (x[0],x[1]))\nsmall_test_predict = small_test.map(lambda x: (x[0],x[1]))\nuser_predictions = small_movies.map(lambda x: (x[0],x[1]))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["small_test_predict.take(3)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\nrank = 8\nnumIterations = 10\nmodel = ALS.train(small_training, rank, numIterations)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["predictions = model.predictAll(small_validation_predict).map(lambda z: ((z[0],z[1]),z[2]))\nrates_and_predictions = small_ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\nMSE = rates_and_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\nprint(\"Mean Squared Error = \" + str(MSE))\n\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#model.save(sc, \"target/tmp/myCollaborativeFilter\")\n#sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["\ntest_predictions = model.predictAll(small_test_predict).map(lambda z: ((z[0],z[1]),z[2]))\ntest_rates_and_predictions = small_test.map(lambda r: ((r[0], r[1]), r[2])).join(test_predictions)\nMSE = test_rates_and_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\nprint(\"Mean Squared Error = \" + str(MSE))\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#https://www.codementor.io/jadianes/building-a-recommender-with-apache-spark-python-example-app-part1-du1083qbw\n'''def get_counts_and_averages(ID_and_ratings_tuple):\n    nratings = len(ID_and_ratings_tuple[1])\n    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)''' #WIP"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["'''complete_data = (small_ratings.map(lambda x: (x[1], x[2])).groupByKey())\ncomplete_data_with_added_data = complete_data.map(get_counts_and_averages)\ntimes_rated_data = complete_data_with_added_data.map(lambda x: (x[0], x[1][0]))''' #WIP"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["times_rated_data.take(3)\n# This shows frequency of movies rated"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["'''def recommend_me_a_movie_or_three(model,my_movie_profile,rating_data=small_ratings,movie_data = small_movies,times_rated_data = times_rated_data,new_user_ID=new_user_ID):\n  rating_data = rating_data.union(my_movie_profile)\n  movies_not_to_include = rating_data.map(lambda x: x[1], my_movie_profile)\n  movies_to_recommend = (rating_data.filter(lambda x: x[0] not in movies_not_to_include).map(lambda x: (new_user_ID, x[0])))\n  user_predictions = model.predictAll(movies_to_recommend)\n  \n  \n  user_predictions_ratings = user_predictions.map(lambda x: (x.product, x.rating))\n  return  user_predictions_ratings''' # WIP"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["small_ratings.take(2)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["\nmy_rated_movies = set([x[1] for x in new_user_ratings])\nmovies_to_recommend = sc.parallelize([m for m in user_predictions.collect() if m[0] not in my_rated_movies])\npredictions = model.predictAll(movies_to_recommend.map(lambda x: (0,x[0])))\nrecommendations = sorted(predictions.collect(), key=lambda x: x[2], reverse=True)\nmovie_names = dict(movies_to_recommend.collect())\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["\npredictions.take(3)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["recommendations[:3]"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["for x in recommendations[:3]:\n  print str(movie_names[x[1]]) + ' Is recommended to me'\n  "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#Big thanks to :\n#https://www.codementor.io/jadianes/building-a-recommender-with-apache-spark-python-example-app-part1-du1083qbw\n#https://github.com/databricks/spark-training/blob/master/machine-learning/python/solution/MovieLensALS.py"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"Project 3 MovieLens (Basic Tutorial)","notebookId":3046387481876367},"nbformat":4,"nbformat_minor":0}
