<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Project 3 MovieLens (Basic Tutorial) - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"serverlessDefaultSparkVersion":"2.2.x-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":14,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":0,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"serverlessClusterProductName":"Serverless Pool","enableTableHandler":false,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"serverlessClustersEnabled":false,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"perClusterAutoterminationEnabled":false,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.11","displayName":"3.0 beta (Scala 2.11)","packageLabel":"spark-image-c29cc066f8182f63337d06fbdb5d0d42656a2456da28d7a6c6eba8ce6829d145","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-f43ddf8ad9acfd71338f5a51345f077173d236016ad7e39ffd6f698403acd4ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.10","displayName":"3.0 beta (Scala 2.10)","packageLabel":"spark-image-3d735029d7732dbd8fa1c01ced355310df03f39534e7b12475ebb9e5551c6a64","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.0.x-scala2.11","displayName":"3.0 beta (Scala 2.11)","packageLabel":"spark-image-c29cc066f8182f63337d06fbdb5d0d42656a2456da28d7a6c6eba8ce6829d145","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.0.x-scala2.10","displayName":"3.0 beta (Scala 2.10)","packageLabel":"spark-image-3d735029d7732dbd8fa1c01ced355310df03f39534e7b12475ebb9e5551c6a64","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":6,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":12,"i3.16xlarge":24,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":3,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1.5,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.48","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":true,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableNonPollingTableCall":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"defaultAutoterminationMin":180,"useDevTierHomePage":true,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","cloud":"AWS","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"enableElasticDisk":false,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"e1fefa9612619b14bdc16008e6f80ee2dffc425f","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableNewJobList":true,"enableNewTableUI":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":false,"enableEBSVolumesUIByTier":false,"singleSignOnComingSoon":false,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","maxAutoterminationMinutes":10000,"autoterminateClustersByDefault":true,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3046387481876367,"name":"Project 3 MovieLens (Basic Tutorial)","language":"python","commands":[{"version":"CommandV1","origId":3046387481876369,"guid":"db5041e8-8a12-4f2d-b204-41dbdf34cd83","subtype":"command","commandType":"auto","position":1.0,"command":"#Two datasets found - A big and small dataset provided by Movielens \ncomplete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\nsmall_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\ndata_path = '/dbfs/FileStore/data'\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498432383025,"submitTime":1498432383019,"finishTime":1498432383095,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":" Find data : I found movieLens","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"066470d5-9c1d-4bf2-9190-70d0fe28ba5d"},{"version":"CommandV1","origId":3046387481876370,"guid":"fd89cee2-5088-43de-8235-7f27c35a71be","subtype":"command","commandType":"auto","position":2.0,"command":"import urllib\nsmall_f = urllib.urlretrieve (small_dataset_url, '/dbfs/FileStore/small.zip')\ncomplete_f = urllib.urlretrieve (complete_dataset_url, '/dbfs/FileStore/complete.zip')\n","commandVersion":0,"state":"finished","results":null,"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;small_dataset_url&apos; is not defined","error":null,"workflows":[],"startTime":0,"submitTime":1498355593920,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Retrieve and store data :  Databricks filesystem","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"866da014-5a22-491d-9916-93e7655e5eb7"},{"version":"CommandV1","origId":3046387481876372,"guid":"ef4d0160-a970-4750-9a24-c63b8b3583dc","subtype":"command","commandType":"auto","position":4.0,"command":"import zipfile\n\nwith zipfile.ZipFile('/dbfs/FileStore/small.zip', \"r\") as z:\n    z.extractall(data_path)\n\nwith zipfile.ZipFile('/dbfs/FileStore/complete.zip', \"r\") as z:\n    z.extractall(data_path)","commandVersion":0,"state":"finished","results":null,"errorSummary":"<span class=\"ansired\">IOError</span>: [Errno 2] No such file or directory: &apos;dbfs:/FileStore/data/ml-latest-small.zip&apos;","error":null,"workflows":[],"startTime":0,"submitTime":1498355092939,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Prepare data : for RecSys","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0dfca1ac-5668-4309-b23b-b0a89c66a5ff"},{"version":"CommandV1","origId":3046387481876373,"guid":"cf8bf142-25f3-48fc-adf9-3a56ce7a9420","subtype":"command","commandType":"auto","position":5.0,"command":"import os\nos.listdir(data_path+'/ml-latest-small')\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">257</span><span class=\"ansired\">]: </span>[&apos;README.txt&apos;, &apos;links.csv&apos;, &apos;movies.csv&apos;, &apos;ratings.csv&apos;, &apos;tags.csv&apos;]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;data_path&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-255-58d7eb5749cc&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">import</span> os<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>os<span class=\"ansiyellow\">.</span>listdir<span class=\"ansiyellow\">(</span>data_path<span class=\"ansiyellow\">+</span><span class=\"ansiblue\">&apos;/ml-latest-small&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;data_path&apos; is not defined</div>","workflows":[],"startTime":1498432388066,"submitTime":1498432388061,"finishTime":1498432388590,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9123886a-ae88-4802-a315-0c2fe2bc93c7"},{"version":"CommandV1","origId":3046387481876374,"guid":"ae03424a-3559-4e0d-96d6-316eca300814","subtype":"command","commandType":"auto","position":6.0,"command":"os.listdir(data_path+'/ml-latest')","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">258</span><span class=\"ansired\">]: </span>\n[&apos;README.txt&apos;,\n &apos;genome-scores.csv&apos;,\n &apos;genome-tags.csv&apos;,\n &apos;links.csv&apos;,\n &apos;movies.csv&apos;,\n &apos;ratings.csv&apos;,\n &apos;tags.csv&apos;]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">OSError</span>: [Errno 2] No such file or directory: &apos;/dbfs/FileStore/dataml-latest&apos;","error":null,"workflows":[],"startTime":1498432390666,"submitTime":1498432390651,"finishTime":1498432390887,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3a3e5c8d-42f8-4c7f-bf31-648ed5de1247"},{"version":"CommandV1","origId":3046387481876375,"guid":"e6c1764e-48d9-4618-940c-a421083f3088","subtype":"command","commandType":"auto","position":7.0,"command":"import pandas as pd\npandas_df = pd.read_csv(\"/dbfs/FileStore/data/ml-latest-small/ratings.csv\", header=0)\nsmall_ratings = sqlContext.createDataFrame(pandas_df).drop('timestamp').rdd.map(tuple)\npandas_df = pd.read_csv(\"/dbfs/FileStore/data/ml-latest-small/movies.csv\",header=0)\nsmall_movies = sqlContext.createDataFrame(pandas_df).drop('genres').rdd.map(tuple)\n\n#I remove timestamp from the ratings file and genres from the movies file while converting csv to RDD.\n\n#While there was no 'real' point to first converting csv into pandas DF to then spark's RDDs; it read much easier for me.\n\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">TypeError</span>: Can not infer schema for type: &lt;type &apos;str&apos;&gt;","error":null,"workflows":[],"startTime":1498430485968,"submitTime":1498430485966,"finishTime":1498430496126,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9e21373e-786d-44d3-bca6-91510ffa2786"},{"version":"CommandV1","origId":1728215777935210,"guid":"6a5051b3-3db7-496f-a4ad-b7c4278b6e7a","subtype":"command","commandType":"auto","position":7.25,"command":"small_ratings.take(3)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">259</span><span class=\"ansired\">]: </span>[(1, 31, 2.5), (1, 1029, 3.0), (1, 1061, 3.0)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498432550170,"submitTime":1498432550165,"finishTime":1498432550343,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ff14e499-ea85-486c-b5d5-62dfb5cade35"},{"version":"CommandV1","origId":1728215777935214,"guid":"d397e2e2-f28e-49c8-b247-d5fa558b7d46","subtype":"command","commandType":"auto","position":7.375,"command":"small_movies.take(2)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">260</span><span class=\"ansired\">]: </span>[(1, u&apos;Toy Story (1995)&apos;), (2, u&apos;Jumanji (1995)&apos;)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498432558410,"submitTime":1498432558395,"finishTime":1498432558530,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bc8fd4a4-7031-4359-8723-6afb6e778ddc"},{"version":"CommandV1","origId":1728215777935209,"guid":"36989095-7441-4ce7-9e28-733a9736b1ef","subtype":"command","commandType":"auto","position":7.5,"command":"new_user_ID = 0\nnew_user_ratings = [\n     (0,1,5), #Toy story\n     (0,2,4), # Jumanji\n     (0,3,3), # Grumpier old men\n     (0,4,2), # Waiting to exhale\n     (0,5,2), # Father of bride pt. 2\n     (0,6,1), # Heat\n     (0,7,5), # Sabrina \n     (0,8,4), #Tom and huck \n     (0,9,3) , # sudden death\n     (0,10,3) # golden eye\n    ]\nsmall_ratings = small_ratings.union(sc.parallelize(new_user_ratings))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498430507098,"submitTime":1498430507083,"finishTime":1498430507168,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"I add my own user profile (userId,movieId,rating) : into the RDD","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"574d9e63-bc81-49a6-b503-3d2d690edab7"},{"version":"CommandV1","origId":3046387481876379,"guid":"8296f679-db1a-4de7-baef-deaed1cf18d9","subtype":"command","commandType":"auto","position":9.0,"command":"small_training,small_validation,small_test = small_ratings.randomSplit([6.0,2.0,2.0],seed = 26)\nsmall_validation_predict = small_validation.map(lambda x: (x[0],x[1]))\nsmall_test_predict = small_test.map(lambda x: (x[0],x[1]))\nuser_predictions = small_movies.map(lambda x: (x[0],x[1]))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;small_ratings&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-2-d5aa58a055e6&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>small_training<span class=\"ansiyellow\">,</span>small_validation<span class=\"ansiyellow\">,</span>small_test <span class=\"ansiyellow\">=</span> small_ratings<span class=\"ansiyellow\">.</span>randomSplit<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">6.0</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">2.0</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">2.0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>seed <span class=\"ansiyellow\">=</span> <span class=\"ansicyan\">26</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> small_validation_predict <span class=\"ansiyellow\">=</span> small_validation<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> small_test_predict <span class=\"ansiyellow\">=</span> small_test<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;small_ratings&apos; is not defined</div>","workflows":[],"startTime":1498430543684,"submitTime":1498430543682,"finishTime":1498430543722,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Create : training / validation / test ","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"48fed898-b139-4afb-a01e-a385ae6d8d8b"},{"version":"CommandV1","origId":3046387481876381,"guid":"c523765c-a28e-4270-b5cb-300a4c3df2b1","subtype":"command","commandType":"auto","position":11.0,"command":"small_test_predict.take(3)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">198</span><span class=\"ansired\">]: </span>[(1, 1029), (1, 1061), (1, 1172)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498430172761,"submitTime":1498430172750,"finishTime":1498430172882,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9976ca70-20d8-4594-93a1-65ea48d37e91"},{"version":"CommandV1","origId":3046387481876385,"guid":"a661dc04-18e0-424e-a44f-6a4cae252304","subtype":"command","commandType":"auto","position":12.0,"command":"from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\nrank = 8\nnumIterations = 10\nmodel = ALS.train(small_training, rank, numIterations)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;small_training&apos; is not defined","error":null,"workflows":[],"startTime":1498430585918,"submitTime":1498430585917,"finishTime":1498430588333,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Train model : { I found rank = 8 to have the lowest MSE}","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9d5490e8-9dd3-431a-8d2d-1bfbdeb753a6"},{"version":"CommandV1","origId":3046387481876386,"guid":"c4d921c6-6acb-4664-b27e-af3359177bc5","subtype":"command","commandType":"auto","position":13.0,"command":"predictions = model.predictAll(small_validation_predict).map(lambda z: ((z[0],z[1]),z[2]))\nrates_and_predictions = small_ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\nMSE = rates_and_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\nprint(\"Mean Squared Error = \" + str(MSE))\n\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Mean Squared Error = 1.47147001854\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 403.0 failed 1 times, most recent failure: Lost task 2.0 in stage 403.0 (TID 611, localhost, executor driver): java.net.SocketException: Socket is closed","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-6-222f4de20452&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>predictions <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>predictAll<span class=\"ansiyellow\">(</span>small_validation_predict<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> z<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>z<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>z<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span>z<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> rates_and_predictions <span class=\"ansiyellow\">=</span> small_ratings<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> r<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>predictions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> MSE <span class=\"ansiyellow\">=</span> rates_and_predictions<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> r<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">-</span> r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">**</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>mean<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansigreen\">print</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Mean Squared Error = &quot;</span> <span class=\"ansiyellow\">+</span> str<span class=\"ansiyellow\">(</span>MSE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/recommendation.pyc</span> in <span class=\"ansicyan\">predictAll</span><span class=\"ansiblue\">(self, user_product)</span>\n<span class=\"ansigreen\">    146</span>         <span class=\"ansigreen\">assert</span> len<span class=\"ansiyellow\">(</span>first<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;user_product should be RDD of (user, product)&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    147</span>         user_product <span class=\"ansiyellow\">=</span> user_product<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> u_p<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>u_p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> int<span class=\"ansiyellow\">(</span>u_p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 148</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>call<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;predict&quot;</span><span class=\"ansiyellow\">,</span> user_product<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    149</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    150</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;1.2.0&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/common.pyc</span> in <span class=\"ansicyan\">call</span><span class=\"ansiblue\">(self, name, *a)</span>\n<span class=\"ansigreen\">    144</span>     <span class=\"ansigreen\">def</span> call<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    145</span>         <span class=\"ansiblue\">&quot;&quot;&quot;Call method of java_model&quot;&quot;&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 146</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> callJavaFunc<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">,</span> getattr<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_java_model<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    147</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    148</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/common.pyc</span> in <span class=\"ansicyan\">callJavaFunc</span><span class=\"ansiblue\">(sc, func, *args)</span>\n<span class=\"ansigreen\">    121</span>     <span class=\"ansiblue\">&quot;&quot;&quot; Call Java Function &quot;&quot;&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    122</span>     args <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>_py2java<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> a<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> a <span class=\"ansigreen\">in</span> args<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 123</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> _java2py<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> func<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>args<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    124</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    125</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o189.predict.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 403.0 failed 1 times, most recent failure: Lost task 2.0 in stage 403.0 (TID 611, localhost, executor driver): java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1937)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2000)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.countApproxDistinctUserProduct(MatrixFactorizationModel.scala:97)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.predict(MatrixFactorizationModel.scala:127)\n\tat org.apache.spark.mllib.api.python.MatrixFactorizationModelWrapper.predict(MatrixFactorizationModelWrapper.scala:32)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n</div>","workflows":[],"startTime":1498423122882,"submitTime":1498423122873,"finishTime":1498423124899,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7d49ecb2-cb28-413f-b468-aa7494016877"},{"version":"CommandV1","origId":3046387481876387,"guid":"8dd9ae4e-ae20-49a4-a0b9-1a72b90bcb46","subtype":"command","commandType":"auto","position":14.0,"command":"#model.save(sc, \"target/tmp/myCollaborativeFilter\")\n#sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498373499675,"submitTime":1498373499681,"finishTime":1498373503198,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Optional : model saving / loading","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f3c51542-e0bc-4cb9-b4b8-c3f2b5b03ca5"},{"version":"CommandV1","origId":3561694898785859,"guid":"271d93a3-d443-487e-8076-0d7a6c7e4174","subtype":"command","commandType":"auto","position":17.0,"command":"\ntest_predictions = model.predictAll(small_test_predict).map(lambda z: ((z[0],z[1]),z[2]))\ntest_rates_and_predictions = small_test.map(lambda r: ((r[0], r[1]), r[2])).join(test_predictions)\nMSE = test_rates_and_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\nprint(\"Mean Squared Error = \" + str(MSE))\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Mean Squared Error = 1.420706156\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1610.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1610.0 (TID 1823, localhost, executor driver): java.net.SocketException: Socket is closed","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-75-40d482cc12e0&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>test_predictions <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>predictAll<span class=\"ansiyellow\">(</span>small_test_predict<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> k<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span>k<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span>k<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> test_rates_and_predictions <span class=\"ansiyellow\">=</span> small_test_predict<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> r<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>test_predictions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> MSE <span class=\"ansiyellow\">=</span> test_rates_and_predictions<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> r<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">-</span> r<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">**</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>mean<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansigreen\">print</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Mean Squared Error = &quot;</span> <span class=\"ansiyellow\">+</span> str<span class=\"ansiyellow\">(</span>MSE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/recommendation.py</span> in <span class=\"ansicyan\">predictAll</span><span class=\"ansiblue\">(self, user_product)</span>\n<span class=\"ansigreen\">    146</span>         <span class=\"ansigreen\">assert</span> len<span class=\"ansiyellow\">(</span>first<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;user_product should be RDD of (user, product)&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    147</span>         user_product <span class=\"ansiyellow\">=</span> user_product<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> u_p<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>u_p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> int<span class=\"ansiyellow\">(</span>u_p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 148</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>call<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;predict&quot;</span><span class=\"ansiyellow\">,</span> user_product<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    149</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    150</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;1.2.0&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/common.py</span> in <span class=\"ansicyan\">call</span><span class=\"ansiblue\">(self, name, *a)</span>\n<span class=\"ansigreen\">    144</span>     <span class=\"ansigreen\">def</span> call<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    145</span>         <span class=\"ansiblue\">&quot;&quot;&quot;Call method of java_model&quot;&quot;&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 146</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> callJavaFunc<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">,</span> getattr<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_java_model<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    147</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    148</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/common.py</span> in <span class=\"ansicyan\">callJavaFunc</span><span class=\"ansiblue\">(sc, func, *args)</span>\n<span class=\"ansigreen\">    121</span>     <span class=\"ansiblue\">&quot;&quot;&quot; Call Java Function &quot;&quot;&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    122</span>     args <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>_py2java<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> a<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> a <span class=\"ansigreen\">in</span> args<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 123</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> _java2py<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> func<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>args<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    124</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    125</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o175.predict.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1610.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1610.0 (TID 1823, localhost, executor driver): java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1937)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2000)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.countApproxDistinctUserProduct(MatrixFactorizationModel.scala:97)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.predict(MatrixFactorizationModel.scala:127)\n\tat org.apache.spark.mllib.api.python.MatrixFactorizationModelWrapper.predict(MatrixFactorizationModelWrapper.scala:32)\n\tat sun.reflect.GeneratedMethodAccessor295.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n</div>","workflows":[],"startTime":1498416795829,"submitTime":1498416795816,"finishTime":1498416797747,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Use model on test data : MSE goes down {not by much}","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a9fc4318-64ef-46e0-9227-9d1c4a2d1010"},{"version":"CommandV1","origId":1728215777935195,"guid":"9aa59b61-c187-48cf-b61e-e16a1af90eb8","subtype":"command","commandType":"auto","position":18.0,"command":"#https://www.codementor.io/jadianes/building-a-recommender-with-apache-spark-python-example-app-part1-du1083qbw\n'''def get_counts_and_averages(ID_and_ratings_tuple):\n    nratings = len(ID_and_ratings_tuple[1])\n    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)''' #WIP","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498424172275,"submitTime":1498424172252,"finishTime":1498424172347,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"WIP : Ignore","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ad6a17ee-974f-45e4-8ce6-5d356c212d5b"},{"version":"CommandV1","origId":1728215777935196,"guid":"a844b395-0b7d-4d14-807d-d988790f2939","subtype":"command","commandType":"auto","position":19.0,"command":"'''complete_data = (small_ratings.map(lambda x: (x[1], x[2])).groupByKey())\ncomplete_data_with_added_data = complete_data.map(get_counts_and_averages)\ntimes_rated_data = complete_data_with_added_data.map(lambda x: (x[0], x[1][0]))''' #WIP","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498424348461,"submitTime":1498424348437,"finishTime":1498424348503,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0a1f8e6a-7c8d-4330-9351-5b52a83dd3e1"},{"version":"CommandV1","origId":1728215777935197,"guid":"81e005e9-0120-4ea7-a21a-ac53375e5892","subtype":"command","commandType":"auto","position":20.0,"command":"times_rated_data.take(3)\n# This shows frequency of movies rated","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">262</span><span class=\"ansired\">]: </span>[(83480, 1), (5120, 6), (1432, 5)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498432871861,"submitTime":1498432871847,"finishTime":1498432871982,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d8fd2720-c155-4e24-9bb0-3ca825ff9eb3"},{"version":"CommandV1","origId":1728215777935198,"guid":"245906fe-1ec8-4e1f-9a77-559820f2d7ef","subtype":"command","commandType":"auto","position":21.0,"command":"'''def recommend_me_a_movie_or_three(model,my_movie_profile,rating_data=small_ratings,movie_data = small_movies,times_rated_data = times_rated_data,new_user_ID=new_user_ID):\n  rating_data = rating_data.union(my_movie_profile)\n  movies_not_to_include = rating_data.map(lambda x: x[1], my_movie_profile)\n  movies_to_recommend = (rating_data.filter(lambda x: x[0] not in movies_not_to_include).map(lambda x: (new_user_ID, x[0])))\n  user_predictions = model.predictAll(movies_to_recommend)\n  \n  \n  user_predictions_ratings = user_predictions.map(lambda x: (x.product, x.rating))\n  return  user_predictions_ratings''' # WIP","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;my_movie_profile&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-97-fb1f917477ca&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">def</span> recommend_me_a_movie_or_three<span class=\"ansiyellow\">(</span>model<span class=\"ansiyellow\">,</span>my_movie_profile<span class=\"ansiyellow\">=</span>my_movie_profile<span class=\"ansiyellow\">,</span>complete_data<span class=\"ansiyellow\">=</span>small_ratings<span class=\"ansiyellow\">,</span>times_rated_data <span class=\"ansiyellow\">=</span> times_rated_data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span>   <span class=\"ansigreen\">pass</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;my_movie_profile&apos; is not defined</div>","workflows":[],"startTime":1498426646655,"submitTime":1498426646655,"finishTime":1498426646727,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Also WIP : Ignore","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8be0ef38-c250-466d-a0cf-afdc08e3ef86"},{"version":"CommandV1","origId":1728215777935205,"guid":"6fdccc24-e51e-4fe4-99f1-1f255b0c59f6","subtype":"command","commandType":"auto","position":22.0,"command":"small_ratings.take(2)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">122</span><span class=\"ansired\">]: </span>[(1, 31, 2.5), (1, 1029, 3.0)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498427982323,"submitTime":1498427982307,"finishTime":1498427982445,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"42c0f528-1de2-4e41-aef2-1e76f2fc5e73"},{"version":"CommandV1","origId":1728215777935202,"guid":"9b194766-3d60-4b5b-9582-31944b069eae","subtype":"command","commandType":"auto","position":23.0,"command":"\nmy_rated_movies = set([x[1] for x in new_user_ratings])\nmovies_to_recommend = sc.parallelize([m for m in user_predictions.collect() if m[0] not in my_rated_movies])\npredictions = model.predictAll(movies_to_recommend.map(lambda x: (0,x[0])))\nrecommendations = sorted(predictions.collect(), key=lambda x: x[2], reverse=True)\nmovie_names = dict(movies_to_recommend.collect())\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">IndexError</span>: tuple index out of range","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-220-a3df6fc86afa&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      2</span> movies_to_recommend <span class=\"ansiyellow\">=</span> sc<span class=\"ansiyellow\">.</span>parallelize<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span>m <span class=\"ansigreen\">for</span> m <span class=\"ansigreen\">in</span> user_predictions<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">if</span> m<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> my_rated_movies<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> predictions <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>predictAll<span class=\"ansiyellow\">(</span>movies_to_recommend<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>recommendations <span class=\"ansiyellow\">=</span> sorted<span class=\"ansiyellow\">(</span>predictions<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> key<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> reverse<span class=\"ansiyellow\">=</span>False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-220-a3df6fc86afa&gt;</span> in <span class=\"ansicyan\">&lt;lambda&gt;</span><span class=\"ansiblue\">(x)</span>\n<span class=\"ansigreen\">      2</span> movies_to_recommend <span class=\"ansiyellow\">=</span> sc<span class=\"ansiyellow\">.</span>parallelize<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span>m <span class=\"ansigreen\">for</span> m <span class=\"ansigreen\">in</span> user_predictions<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">if</span> m<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> my_rated_movies<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> predictions <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>predictAll<span class=\"ansiyellow\">(</span>movies_to_recommend<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>recommendations <span class=\"ansiyellow\">=</span> sorted<span class=\"ansiyellow\">(</span>predictions<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> key<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> reverse<span class=\"ansiyellow\">=</span>False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">IndexError</span>: tuple index out of range</div>","workflows":[],"startTime":1498431802809,"submitTime":1498431802804,"finishTime":1498431803542,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Use best model : Recommend me some movies","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"af7f905f-d827-4b5c-b73f-eca85bdd26c3"},{"version":"CommandV1","origId":1728215777935203,"guid":"ed37430f-404c-4684-8e2d-a825578e649c","subtype":"command","commandType":"auto","position":24.0,"command":"\npredictions.take(3)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">222</span><span class=\"ansired\">]: </span>\n[Rating(user=0, product=45208, rating=0.478406016697943),\n Rating(user=0, product=3456, rating=7.523098731867851),\n Rating(user=0, product=3272, rating=5.130147786067621)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-157-655826532fe6&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">1</span>\n<span class=\"ansiyellow\">    my_recommendations = sorted(recommendations, key-lambda x: x[0] )</span>\n<span class=\"ansigrey\">                                                          ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax\n</div>","workflows":[],"startTime":1498430636000,"submitTime":1498430635989,"finishTime":1498430636122,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bf74d90d-5d92-4aac-943f-9612ee3e5152"},{"version":"CommandV1","origId":1728215777935204,"guid":"e50c0cbc-b16f-4739-9e7d-41437983b258","subtype":"command","commandType":"auto","position":25.0,"command":"recommendations[:3]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">231</span><span class=\"ansired\">]: </span>\n[Rating(user=0, product=3705, rating=10.830879585924825),\n Rating(user=0, product=175, rating=10.634541035567633),\n Rating(user=0, product=53123, rating=10.5464838066478)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;list&apos; object has no attribute &apos;take&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-229-d88acd8bdb49&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>recommendations<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;list&apos; object has no attribute &apos;take&apos;</div>","workflows":[],"startTime":1498431032530,"submitTime":1498431032530,"finishTime":1498431032568,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a1482273-8ad2-4293-833f-520968b298ac"},{"version":"CommandV1","origId":1728215777935211,"guid":"1a43e292-c4ed-47e0-925f-21cc53243a52","subtype":"command","commandType":"auto","position":25.5,"command":"for x in recommendations[:3]:\n  print str(movie_names[x[1]]) + ' Is recommended to me'\n  ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Bird on a Wire (1990) Is recommended to me\nKids (1995) Is recommended to me\nOnce (2006) Is recommended to me\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">TypeError</span>: &apos;dict&apos; object is not callable","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-252-02addb4fa37c&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">for</span> x <span class=\"ansigreen\">in</span> recommendations<span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">:</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\">   </span><span class=\"ansigreen\">print</span> str<span class=\"ansiyellow\">(</span>movie_names<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">+</span> <span class=\"ansiblue\">&apos;Is recommended to you&apos;</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: &apos;dict&apos; object is not callable</div>","workflows":[],"startTime":1498433002074,"submitTime":1498433002068,"finishTime":1498433002144,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"Here are the top 3 movies recommended to me; using ALS method by mllib / spark / databricks","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d3dd411d-cb25-42be-99a0-ced84ba5e3be"},{"version":"CommandV1","origId":1728215777935215,"guid":"9ae39ba1-d9c0-4bed-8f09-967457cae56c","subtype":"command","commandType":"auto","position":26.5,"command":"#Big thanks to :\n#https://www.codementor.io/jadianes/building-a-recommender-with-apache-spark-python-example-app-part1-du1083qbw\n#https://github.com/databricks/spark-training/blob/master/machine-learning/python/solution/MovieLensALS.py","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498433128453,"submitTime":1498433128439,"finishTime":1498433128524,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"15d2bd39-7735-4995-84df-496055bf7da7"}],"dashboards":[],"guid":"37094ba1-7d82-49e2-8969-87840d09cae1","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
