{"cells":[{"cell_type":"code","source":["\n'''\nATTENTION: USE THIS LINK for better view of this project https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7294676898699104/3265892471185373/5612764032532480/latest.html\nMichael Muller\n\nObjective : Create a recommender system on a big dataset\n\nBusiness Goal : Increase CTR\n\nI searched quite a bit for inspiration on this recommender system; \n\nI was tired of databricks giving me OOM connection refusals, which would cause my entire cluster to lose all data;\nDuring my frustration I read an article on evolution based recommender systems: http://www0.cs.ucl.ac.uk/staff/ucacpjb/UJBEC3.pdf\nUjjin and Bently formatted their data with quite a bit of memory overlap and to compensate; they selected samples of their data so their methods would be less computationally expensive. \nReading more articles on comparing different recSys algorithms, Singular value decomposition was treated with praise; but not recommended because of the computational burden.\nIn my DATA 643 Project 4, I had to reduce my dataset by around 70% (and retry the engine several times with crashing)\n\nMy recommender System takes a different approach from conventional recommender systems in two ways\n\n1. To significantly decrease computational expenses; I sample from my dataset using intentional bias : Geographical location and age.\nMy recommender algorithms are modeled through only the users of closest geographical euclidean distance to each other, who fall in the same two generational categories of young and old as well.\n\n2. Depending on samples; over and undersampling may pigeonhole user recommendations due to implementing the wrong model.\nMy recommender algorithms are plenty; I use SVD+SGM,SVD,ALS and Cosine based item-item models to create a plethora of recommendations all with less computations than my previous systems; for around the same RMSE.\n\nHopefully having a multitude of algorithms deriving recommendations will improve an apps 'click through rate' in such a way that serendipity might.\n\n\n'''\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import urllib\nimport zipfile\ngeo_url = 'https://raw.githubusercontent.com/parastyle/DATA-643/master/stateGeoInfo.csv'\nbook_url = 'http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip'\ndata_path = '/dbfs/FileStore/data'\nbook_crossing = urllib.urlretrieve (book_url, 'book_crossing.zip')\nbook_crossing = urllib.urlretrieve (geo_url, data_path+'/stateGeoInfo.csv')\nwith zipfile.ZipFile('/dbfs/FileStore/book_crossing.zip', \"r\") as z:\n    z.extractall(data_path)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import os\nos.listdir(data_path)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import pandas as pd\ngeo_df = pd.read_csv('/dbfs/FileStore/data/stateGeoInfo.csv',header=0,sep=',',error_bad_lines=False,quotechar = \"'\",index_col=False)\ngeo_df = geo_df.apply(lambda x: x.astype(str).str.lower()).drop(['slug','area','is_state','country','is_lower48'],axis=1)\ngeo_df['name'] = geo_df['name'].replace('district of columbia', 'dc')\nstates = geo_df['name']"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["geo_df.head()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import pandas as pd\n\ncolu = [\"ISBN\",\"Book-Title\",\"Book-Author\",\"Year-Of-Publication\",\"Publisher\",\"Image-URL-M\"]\n\nratings_df = pd.read_csv(\"/dbfs/FileStore/data/BX-Book-Ratings.csv\", header=0,sep=';', error_bad_lines=False,dtype={\"User-id\": int, \"IBSN\": object, \"Book-Rating\": int})\nbooks_df = pd.read_csv(\"/dbfs/FileStore/data/BX-Books.csv\", header=0,sep=';', error_bad_lines=False,usecols = colu)\nusers_df = pd.read_csv(\"/dbfs/FileStore/data/BX-Users.csv\", header=0,sep=';', error_bad_lines=False)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["complete_df=ratings_df.merge(books_df)\ncomplete_df=complete_df.merge(users_df)\ncomplete_df['itemID'] = complete_df['ISBN'].astype('category').cat.codes\ncomplete_df['Age'] = complete_df['Age'].apply(lambda x: 'young' if x<30 else 'old')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["complete_df.columns"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["\nusa_df = complete_df[complete_df['Location'].str.contains('usa',regex=True,na=False)]\nusa_df['Location'] = usa_df['Location'].str.extract('(?<=, )(.+?),')\nusa_df['Location'] = usa_df['Location'].replace(['acworth','ga','ga.'],'georgia')\nusa_df['Location'] = usa_df['Location'].replace(['ae','ap'],'n/a')\nusa_df['Location'] = usa_df['Location'].replace(['ca'],'california')\nusa_df['Location'] = usa_df['Location'].replace(['rhode island'],'dc')"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["counts = usa_df['Location'].value_counts()\nusa_df = usa_df[usa_df['Location'].isin(counts[counts>200].index)]\nusa_df['Location'].unique()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["fig = usa_df['Location'].value_counts().plot(kind='bar')\ndisplay(fig.figure)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#9000 ratings per state average\ncounts[:50].median()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#106,000 ratings in california\nlen(usa_df[usa_df['Location']=='california'])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["len(usa_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["explicit_ratings_df = usa_df[usa_df[\"Book-Rating\"] != 0]\nimplicit_ratings_df = usa_df[usa_df[\"Book-Rating\"] == 0]\n\nunique_users_by_state = []\nfor z in states:\n  trivial = explicit_ratings_df[explicit_ratings_df['Location']==z]\n  unique_users_by_state.append(len(trivial['User-ID'].value_counts()))\n  \n  # 874 users per state on average\nmean_users_per_state = sum(unique_users_by_state)/(len(unique_users_by_state)-1) #-1 for n/a\nmean_users_per_state"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Distance metric 'eyed' from http://www.worldatlas.com/webimage/countrys/namerica/usstates/uslandst.htm\n\nfrom geopy.distance import vincenty\n\nall_combinations = []\nsampling_states = {}\nstate_frequency = []\n\nfor index, row in geo_df.iterrows():\n  all_combinations = []\n  objective = 0\n  states_to_use = []\n  origin_latlong = (float(row[2]),float(row[3]))\n  for state in states:\n    dest_latlong = (float(geo_df[geo_df['name']==state]['latitude']), float(geo_df[geo_df['name']==state]['longitude']))\n    miles =  float(vincenty(origin_latlong,dest_latlong).miles)\n    population = int(geo_df[geo_df['name']==state]['population'])\n    if miles <= 1500:\n      all_combinations.append((state,miles))#add 'population' to parameters if you live in a perfect world with equally distributed ratings per state according to populations.\n  all_combinations = sorted(all_combinations,key=lambda state: state[1])\n  '''for x in all_combinations: \n    objective += x[2]\n    if objective > 100000000:          # Uncomment if you live in a perfect...\n      break\n    states_to_use.append(x[0])'''\n  sampling_states[row[0]] = all_combinations #Replace with states_to_use if you live in a perfect world with equally distributed ratings per state according to populations."],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Euclidean distance between center of Alabama and center of ?\nsampling_states['alabama'][:10]"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["sampling_states['new york'][:10]"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["import random\nrandom.seed(136)\nrandom_user = explicit_ratings_df['User-ID'].unique()[random.randint(1,4000)]\nuser = explicit_ratings_df[explicit_ratings_df['User-ID']==random_user]\nuser_area = str(user['Location'].unique()[0])\nuser_age = str(user['Age'].unique()[0])\nuser.head()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["selected_states = []\nuser_pool = []\nfor area in sampling_states[user_area]:\n  print area\n  theState = explicit_ratings_df[explicit_ratings_df['Location']==area[0]]\n  filterState = theState[theState['Age']==user_age]\n  user_pool.append(len(filterState['User-ID'].value_counts()))\n  selected_states.append(area[0])\n  print sum(user_pool)\n  if sum(user_pool) >= (mean_users_per_state)*(4):\n    break"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["selected_states"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["str_df = explicit_ratings_df[explicit_ratings_df['Location']==selected_states[0]].astype(str)\nstr_df = str_df[str_df['Age']==user_age]\nfocused_rdd = sqlContext.createDataFrame(str_df).rdd.map(lambda x: (x[0],x[10],x[2]))\n\nfor state_data in selected_states[1:]:\n  str_df = explicit_ratings_df[explicit_ratings_df['Location']==state_data].astype(str)\n  str_df = str_df[str_df['Age']==user_age]\n  ratings_rdd_1 = sqlContext.createDataFrame(str_df).rdd.map(lambda x: (x[0],x[10],x[2]))\n  focused_rdd = focused_rdd.union(ratings_rdd_1)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["training,test = focused_rdd.randomSplit([7.0,3.0],seed = 66)\ntest_blank = test.map(lambda x: (x[0],x[1]))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["training.take(3)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\nranks = [3,16,19,22,32]\nnumIterations = 10\nMSElist = []\nfor rank in ranks:\n  model = ALS.train(training, rank, numIterations)\n  predictions = model.predictAll(test_blank).map(lambda z: ((z[0],z[1]),z[2]))\n  rates_and_predictions = test.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n  MSE = rates_and_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n  MSElist.append(MSE)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["for MSE in MSElist:\n  print MSE**.5"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["top_3_als_stats = model.recommendProducts(212579,3)\ntop_3_als = []\ntop_3_als_scores = []\nfor xyz in top_3_als_stats:\n  top_3_als.append(xyz[1])\n  top_3_als_scores.append(xyz[2])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["import graphlab\ngraphlab.product_key.set_product_key('2599-A8A1-ABC0-9247-6D39-351C-6CB4-419B')\nfocused_df = focused_rdd.toDF().toPandas().astype(int)\nfocused_df.rename(columns={'_1':'user_id','_2':'item_id','_3':'rating'},inplace=True)\nusers_profile = graphlab.SFrame(focused_df[focused_df['user_id']==212579])\ndata_SFrame = graphlab.SFrame(focused_df[focused_df['user_id']!=212579])"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["focused_df"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["data_SFrame = graphlab.SFrame(focused_df)\ntraining_data_SFrame, testing_data_SFrame = graphlab.recommender.util.random_split_by_user(data_SFrame,user_id='user_id', item_id='item_id',item_test_proportion=.8)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["training_data_SFrame"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["factorizationModel = graphlab.recommender.create(training_data_SFrame,user_id='user_id',item_id='item_id',target='rating',ranking=False)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["factorizationModel.recommend(users=[212579],new_user_data=users_profile)[:3]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["top_3_SGD = factorizationModel.recommend(users=[212579],new_user_data=users_profile)[:3]['item_id'] \ntop_3_SGD_scores = factorizationModel.recommend(users=[212579],new_user_data=users_profile)[:3]['score']\ntop_3_SGD\ntop_3_SGD_scores"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["ism = graphlab.recommender.item_similarity_recommender.create(training_data_SFrame,user_id='user_id', item_id='item_id', target='rating', similarity_type='cosine',verbose=True,only_top_k = 5 )"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["top_3_cosine = ism.recommend(users=[212579],k=86,new_user_data=users_profile)\ntop_3_cosine_item = top_3_cosine[:3]['item_id']\ntop_3_cosine"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["top_3_cosine_score = top_3_cosine['score'][:3]\ntop_3_cosine = top_3_cosine['item_id'][:3]"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from surprise import SVD\nfrom surprise import Dataset\nfrom surprise import evaluate, print_perf\nfrom surprise import Reader"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["focused_df.to_csv('/dbfs/FileStore/data/focused_df_svd.csv',sep=';',index=False)\nbackIn = Reader(rating_scale=(1,10),sep=';',line_format=('user item rating'),skip_lines=1)\ndata = Dataset.load_from_file('/dbfs/FileStore/data/focused_df_svd.csv',reader=backIn)\ndata.split(n_folds=5)\nalgo = SVD()\nperf = evaluate(algo, data, measures=['RMSE', 'MAE'])"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["top_list = pd.Series(list(top_3_als) + list(top_3_cosine) + list(top_3_SGD))\ntop_list_score= pd.Series(list(top_3_als_scores) + list(top_3_cosine_score) + list(top_3_SGD_scores))\nfor x,y in zip(top_list,top_list_score):\n  print(algo.predict(uid=212579,iid=x,r_ui=y))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["images = []\nfor x in top_list:\n  print 'Based off users in your area and generation, we recommend :' + str(complete_df[complete_df['itemID'].isin([x])].iloc[0]['Book-Title'] + 'by ' + str(complete_df[complete_df['itemID'].isin([21344])].iloc[0]['Book-Author'])+'\\n')\n  images.append(str(complete_df[complete_df['itemID'].isin([x])].iloc[0]['Image-URL-M']))\n  "],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["  displayHTML(\"<img src =\"+ images[0] +\">\\n\"+\"<img src =\"+ images[1] +\">\\n\"+\"<img src =\"+ images[2] +\">\\n\"+\"<img src =\"+ images[3] +\">\\n\"+\"<img src =\"+ images[4] +\">\\n\"+\"<img src =\"+ images[5] +\">\\n\"+\"<img src =\"+ images[6] +\">\\n\"+\"<img src =\"+ images[7] +\">\\n\"+\"<img src =\"+ images[8] +\">\\n\")"],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"Final RecSys","notebookId":3265892471185373},"nbformat":4,"nbformat_minor":0}
